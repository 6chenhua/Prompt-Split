"""
å˜é‡æå–æ¨¡å—
é‡æ„åç‰ˆæœ¬ï¼šä½¿ç”¨å…¬å…±å·¥å…·ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
"""

import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

# å¯¼å…¥å…¬å…±å·¥å…·å’Œé‡æ„åçš„LLMå®¢æˆ·ç«¯
from common_utils import FileUtils, TextProcessor, JSONProcessor, LogUtils, ConfigUtils
from LLMTool import LLMApiClient


def call_llm(chunk: str, idx: int, llm_client: LLMApiClient = None, sys_prompt: str = None) -> tuple:
    """
    è°ƒç”¨LLMå¤„ç†å•ä¸ªåˆ†å—
    
    Args:
        chunk: æ–‡æœ¬å—
        idx: å—ç´¢å¼•
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        sys_prompt: ç³»ç»Ÿæç¤ºè¯
        
    Returns:
        (ç´¢å¼•, å“åº”å†…å®¹) å…ƒç»„
    """
    LogUtils.log_info(f"å¼€å§‹å¤„ç†æ–‡æœ¬å— {idx}")
    
    try:
        # å¦‚æœæ²¡æœ‰æä¾›å®¢æˆ·ç«¯ï¼Œåˆ›å»ºä¸€ä¸ª
        if llm_client is None:
            llm_client = LLMApiClient()
        
        # å¦‚æœæ²¡æœ‰æä¾›ç³»ç»Ÿæç¤ºï¼Œè¯»å–é»˜è®¤æ–‡ä»¶
        if sys_prompt is None:
            sys_prompt = FileUtils.read_file('my_prompts/extract_var_v6.txt')
            if not sys_prompt:
                LogUtils.log_error("æ— æ³•è¯»å–ç³»ç»Ÿæç¤ºæ–‡ä»¶ extract_var_v6.txt")
                return idx, ""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": chunk},
        ]
        
        # è°ƒç”¨LLM
        content = llm_client.call(messages)
        
        if content:
            LogUtils.log_success(f"æ–‡æœ¬å— {idx} å¤„ç†æˆåŠŸ")
            return idx, content
        else:
            LogUtils.log_warning(f"æ–‡æœ¬å— {idx} è¿”å›ç©ºå“åº”")
            return idx, ""
            
    except Exception as e:
        LogUtils.log_error(f"å¤„ç†æ–‡æœ¬å— {idx} å¤±è´¥: {e}")
        return idx, ""


def process_chunks_concurrently(chunks: List[str], max_workers: int = 5, 
                               llm_client: LLMApiClient = None) -> List[str]:
    """
    å¹¶å‘å¤„ç†æ–‡æœ¬åˆ†å—ï¼Œä»LLMå›å¤ä¸­æå–å˜é‡å
    
    Args:
        chunks: æ–‡æœ¬å—åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘æ•°
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        å»é‡åçš„å˜é‡ååˆ—è¡¨
    """
    if not chunks:
        LogUtils.log_warning("æ²¡æœ‰æ–‡æœ¬å—éœ€è¦å¤„ç†")
        return []
    
    # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
    config = ConfigUtils.get_config()
    max_workers = min(max_workers, config.get('max_workers', 5))
    
    # å¦‚æœæ²¡æœ‰æä¾›å®¢æˆ·ç«¯ï¼Œåˆ›å»ºä¸€ä¸ª
    if llm_client is None:
        llm_client = LLMApiClient()
    
    # è¯»å–ç³»ç»Ÿæç¤ºï¼ˆä¸€æ¬¡è¯»å–ï¼Œæ‰€æœ‰å—å…±ç”¨ï¼‰
    sys_prompt = FileUtils.read_file('my_prompts/extract_var_v6.txt')
    if not sys_prompt:
        LogUtils.log_error("æ— æ³•è¯»å–ç³»ç»Ÿæç¤ºæ–‡ä»¶")
        return []
    
    LogUtils.log_info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå¹¶å‘æ•°: {max_workers}")
    
    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [
            executor.submit(call_llm, chunk, i, llm_client, sys_prompt) 
            for i, chunk in enumerate(chunks)
        ]
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(futures):
            idx, result_str = future.result()
            LogUtils.log_info(f"æ”¶åˆ°æ–‡æœ¬å— {idx} çš„å¤„ç†ç»“æœ")
            
            if result_str:
                # ä½¿ç”¨å…¬å…±JSONå¤„ç†å™¨æå–å˜é‡
                variables = JSONProcessor.extract_variables_from_json(result_str)
                results[idx] = variables
                LogUtils.log_success(f"æ–‡æœ¬å— {idx} æå–åˆ° {len(variables)} ä¸ªå˜é‡")
            else:
                results[idx] = []
                LogUtils.log_warning(f"æ–‡æœ¬å— {idx} æœªæå–åˆ°å˜é‡")
    
    # ä¿è¯è¾“å‡ºé¡ºåºå’Œè¾“å…¥é¡ºåºä¸€è‡´
    ordered_results = [results.get(i, []) for i in range(len(chunks))]
    
    # æ‰å¹³åŒ–åˆ—è¡¨å¹¶å»é‡
    all_variable_names = [name for sublist in ordered_results for name in sublist]
    unique_variables = list(set(all_variable_names))
    
    LogUtils.log_success(f"å¹¶å‘å¤„ç†å®Œæˆï¼Œå…±æå–åˆ° {len(unique_variables)} ä¸ªå”¯ä¸€å˜é‡")
    return unique_variables


def post_process(nl_with_var: str, llm_client: LLMApiClient = None) -> str:
    """
    åå¤„ç†å˜é‡æ ‡è®°æ–‡æœ¬
    
    Args:
        nl_with_var: æ ‡è®°äº†å˜é‡çš„æ–‡æœ¬
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        å¤„ç†åçš„æ–‡æœ¬
    """
    LogUtils.log_info("å¼€å§‹åå¤„ç†å˜é‡æ ‡è®°")
    
    try:
        # è¯»å–åå¤„ç†æç¤ºè¯
        prompt_template = FileUtils.read_file('my_prompts/post_process_variable_v2.txt')
        if not prompt_template:
            LogUtils.log_error("æ— æ³•è¯»å–åå¤„ç†æç¤ºæ–‡ä»¶")
            return nl_with_var
        
        # æ›¿æ¢å ä½ç¬¦
        prompt = prompt_template.replace("{{prompt_with_var}}", nl_with_var)
        
        # å¦‚æœæ²¡æœ‰æä¾›å®¢æˆ·ç«¯ï¼Œåˆ›å»ºä¸€ä¸ª
        if llm_client is None:
            llm_client = LLMApiClient()
        
        # è°ƒç”¨LLM
        messages = [{"role": "system", "content": prompt}]
        res = llm_client.call(messages)
        
        if not res:
            LogUtils.log_warning("åå¤„ç†è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨åŸæ–‡æœ¬")
            return nl_with_var
        
        # å°è¯•å¤šç§æ–¹å¼æå–å’Œè§£æJSON
        processed_text = nl_with_var
        
        # æ–¹æ³•1: ä½¿ç”¨LLMå®¢æˆ·ç«¯çš„JSONæå–
        json_str = llm_client.extract_json_string(res)
        if json_str:
            try:
                processed_data = json.loads(json_str)
                if 'cleaned_text' in processed_data:
                    processed_text = processed_data['cleaned_text']
                    LogUtils.log_success("å˜é‡åå¤„ç†å®Œæˆ")
                    return processed_text
            except json.JSONDecodeError as e:
                LogUtils.log_warning(f"æ–¹æ³•1 JSONè§£æå¤±è´¥: {e}")
        
        # æ–¹æ³•2: ç›´æ¥å°è¯•è§£ææ•´ä¸ªå“åº”
        try:
            processed_data = json.loads(res)
            if 'cleaned_text' in processed_data:
                processed_text = processed_data['cleaned_text']
                LogUtils.log_success("å˜é‡åå¤„ç†å®Œæˆ")
                return processed_text
        except json.JSONDecodeError:
            LogUtils.log_info("æ–¹æ³•2 JSONè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
        
        # æ–¹æ³•3: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONå—
        import re
        json_pattern = r'\{[^{}]*"cleaned_text"[^{}]*\}'
        matches = re.findall(json_pattern, res, re.DOTALL)
        for match in matches:
            try:
                processed_data = json.loads(match)
                if 'cleaned_text' in processed_data:
                    processed_text = processed_data['cleaned_text']
                    LogUtils.log_success("å˜é‡åå¤„ç†å®Œæˆ")
                    return processed_text
            except json.JSONDecodeError:
                continue
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æœ¬
        LogUtils.log_warning("æ‰€æœ‰JSONè§£ææ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        LogUtils.log_success("å˜é‡åå¤„ç†å®Œæˆ")
        return processed_text
            
    except Exception as e:
        LogUtils.log_error(f"åå¤„ç†å¤±è´¥: {e}")
        return nl_with_var


def extract_variables_from_text(text: str, chunk_size: int = None, max_workers: int = None) -> Dict[str, Any]:
    """
    ä»æ–‡æœ¬ä¸­æå–å˜é‡çš„å®Œæ•´æµç¨‹
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
        max_workers: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        åŒ…å«å˜é‡å’Œå¤„ç†ç»“æœçš„å­—å…¸
    """
    LogUtils.log_step("å˜é‡æå–", "å¼€å§‹å®Œæ•´çš„å˜é‡æå–æµç¨‹")
    
    if not text or not text.strip():
        LogUtils.log_error("è¾“å…¥æ–‡æœ¬ä¸ºç©º")
        return {"error": "è¾“å…¥æ–‡æœ¬ä¸ºç©º"}
    
    try:
        # è·å–é…ç½®
        config = ConfigUtils.get_config()
        chunk_size = chunk_size or config.get('chunk_size', 500)
        max_workers = max_workers or config.get('max_workers', 5)
        
        # ä½¿ç”¨å…¬å…±æ–‡æœ¬å¤„ç†å™¨åˆ†å‰²æ–‡æœ¬
        LogUtils.log_info(f"ä½¿ç”¨åˆ†å—å¤§å°: {chunk_size}")
        chunks = TextProcessor.split_text_by_length(text, chunk_size)
        LogUtils.log_info(f"æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = LLMApiClient()
        
        # å¹¶å‘å¤„ç†æå–å˜é‡
        variables = process_chunks_concurrently(chunks, max_workers, llm_client)
        
        if not variables:
            LogUtils.log_warning("æœªæå–åˆ°ä»»ä½•å˜é‡")
            return {
                "variables": [],
                "original_text": text,
                "text_with_vars": text,
                "chunks_count": len(chunks)
            }
        
        # å°†å˜é‡æ ‡è®°åˆ°åŸæ–‡ä¸­
        LogUtils.log_info("å¼€å§‹æ ‡è®°å˜é‡åˆ°åŸæ–‡")
        text_with_vars = text
        for var in variables:
            text_with_vars = text_with_vars.replace(var, "{" + var + "}")
        
        # åå¤„ç†
        LogUtils.log_info("å¼€å§‹åå¤„ç†")
        processed_text = post_process(text_with_vars, llm_client)
        
        # æ¸…ç†æ–‡æœ¬
        processed_text = TextProcessor.clean_text(processed_text)
        
        result = {
            "variables": variables,
            "original_text": text,
            "text_with_vars": processed_text,
            "chunks_count": len(chunks),
            "stats": {
                "total_variables": len(variables),
                "chunks_processed": len(chunks),
                "chunk_size_used": chunk_size,
                "max_workers_used": max_workers
            }
        }
        
        LogUtils.log_success(f"å˜é‡æå–å®Œæˆï¼Œå…±æå– {len(variables)} ä¸ªå˜é‡")
        return result
        
    except Exception as e:
        LogUtils.log_error(f"å˜é‡æå–æµç¨‹å¤±è´¥: {e}")
        return {"error": str(e)}


def save_extraction_result(result: Dict[str, Any], output_dir: str = "output") -> bool:
    """
    ä¿å­˜å˜é‡æå–ç»“æœ
    
    Args:
        result: æå–ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ä¿å­˜å˜é‡åˆ—è¡¨
        variables_file = os.path.join(output_dir, "extracted_variables.json")
        variables_data = {
            "variables": result.get("variables", []),
            "stats": result.get("stats", {}),
            "timestamp": json.dumps(json.datetime.now().isoformat()) if hasattr(json, 'datetime') else None
        }
        
        if FileUtils.save_json(variables_file, variables_data):
            LogUtils.log_success(f"å˜é‡åˆ—è¡¨å·²ä¿å­˜åˆ° {variables_file}")
        
        # ä¿å­˜æ ‡è®°å˜é‡çš„æ–‡æœ¬
        text_file = os.path.join(output_dir, "text_with_variables.txt")
        if FileUtils.save_file(text_file, result.get("text_with_vars", "")):
            LogUtils.log_success(f"æ ‡è®°æ–‡æœ¬å·²ä¿å­˜åˆ° {text_file}")
        
        return True
        
    except Exception as e:
        LogUtils.log_error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•° - æä¾›å‘½ä»¤è¡Œæ¥å£"""
    import sys
    
    # ç®€å•çš„å‘½ä»¤è¡Œå‚æ•°å¤„ç†
    input_file = sys.argv[1] if len(sys.argv) > 1 else 'nl_prompt.txt'
    
    LogUtils.log_step("å˜é‡æå–å·¥å…·", f"å¤„ç†æ–‡ä»¶: {input_file}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not FileUtils.read_file(input_file):
        LogUtils.log_error(f"æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶: {input_file}")
        return
    
    # è¯»å–æ–‡æœ¬
    text = FileUtils.read_file(input_file)
    LogUtils.log_info(f"å·²è¯»å–æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
    
    # æå–å˜é‡
    result = extract_variables_from_text(text)
    
    if "error" in result:
        LogUtils.log_error(f"æå–å¤±è´¥: {result['error']}")
        return
    
    # æ˜¾ç¤ºç»“æœ
    variables = result.get("variables", [])
    print(f"\nâœ… æå–å®Œæˆï¼å…±æ‰¾åˆ° {len(variables)} ä¸ªå˜é‡ï¼š")
    for i, var in enumerate(variables, 1):
        print(f"  {i}. {var}")
    
    # ä¿å­˜ç»“æœ
    if save_extraction_result(result):
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ° output/ ç›®å½•")
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    stats = result.get("stats", {})
    for key, value in stats.items():
        print(f"  - {key}: {value}")


if __name__ == '__main__':
    main()


