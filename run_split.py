import json
import os
import re
from typing import Dict, Any

# å¯¼å…¥ç°æœ‰çš„åŠŸèƒ½æ¨¡å—
from extract_variable import process_chunks_concurrently, split_text_by_length, post_process
from nl2cnlp import batch_transform_cnlp

# å¯¼å…¥first_split.pyä¸­çš„ç›¸å…³ç»„ä»¶
from first_spilit import (
    generate_mermaid_messages, 
    split_messages, 
    sub_prompt_messages,
    generate_mermaid_prompt,
    split_subsystem_prompt_v3,
    generate_sub_prompt,
    llm_client
)


class PromptSplitPipeline:
    """
    æç¤ºè¯æ‹†åˆ†æµç¨‹ç¼–æ’å™¨
    ç›´æ¥è°ƒç”¨ç°æœ‰å‡½æ•°ï¼Œä¸“æ³¨äºæµç¨‹æ§åˆ¶
    """
    
    def __init__(self):
        pass
    
    def read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            return ""
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return ""
    
    def save_file(self, file_path: str, content: str):
        """ä¿å­˜æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… å·²ä¿å­˜åˆ° {file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    def save_json(self, file_path: str, data: Dict):
        """ä¿å­˜JSONæ•°æ®"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜JSONåˆ° {file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    def step1_extract_variables(self, original_text: str) -> Dict[str, Any]:
        """
        ç¬¬ä¸€æ­¥ï¼šæå–å˜é‡
        è°ƒç”¨ extract_variable.py ä¸­çš„ç°æœ‰å‡½æ•°
        """
        print("=" * 50)
        print("ğŸ” ç¬¬ä¸€æ­¥ï¼šå¼€å§‹æå–å˜é‡...")
        
        if not original_text:
            return {"error": "è¾“å…¥æ–‡æœ¬ä¸ºç©º"}
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ–‡æœ¬åˆ†å‰²å‡½æ•°
            chunks = split_text_by_length(original_text)
            print(f"ğŸ“„ æ–‡æœ¬å·²åˆ‡å‰²ä¸º {len(chunks)} ä¸ªå—")
            
            # ä½¿ç”¨ç°æœ‰çš„å¹¶å‘å¤„ç†å‡½æ•°æå–å˜é‡
            variables = process_chunks_concurrently(chunks)
            print(f"ğŸ¯ æå–åˆ° {len(variables)} ä¸ªå˜é‡: {variables}")
            
            # å°†å˜é‡æ ‡è®°åˆ°åŸæ–‡ä¸­
            text_with_vars = original_text
            for var in variables:
                text_with_vars = text_with_vars.replace(var, "{" + var + "}")
            
            # ä½¿ç”¨ç°æœ‰çš„åå¤„ç†å‡½æ•°
            try:
                processed_text = post_process(text_with_vars)
                print("âœ¨ å˜é‡åå¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ åå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ‡è®°æ–‡æœ¬: {e}")
                processed_text = text_with_vars
            
            result = {
                "variables": variables,
                "original_text": original_text,
                "text_with_vars": processed_text,
                "chunks_count": len(chunks)
            }
            
            print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆï¼Œæå–åˆ° {len(variables)} ä¸ªå˜é‡")
            return result
            
        except Exception as e:
            error_msg = f"å˜é‡æå–å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg}
    
    def generate_mermaid_content(self, text: str) -> str:
        """
        ç”ŸæˆMermaidæµç¨‹å›¾
        ä¿®æ­£first_split.pyä¸­gen_mermaid_contentå‡½æ•°çš„é€»è¾‘
        """
        try:
            # åˆ›å»ºåŒ…å«ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯åˆ—è¡¨
            messages = generate_mermaid_messages.copy()
            messages.append({"role": "user", "content": text})
            
            # è°ƒç”¨LLMç”Ÿæˆæµç¨‹åˆ†æå’Œmermaidå›¾
            response = llm_client.call(messages, "gpt-5-mini")
            
            # ä»å“åº”ä¸­æå–mermaidå›¾
            pattern = r"```mermaid(.*?)```"
            matches = re.findall(pattern, response, re.DOTALL)
            
            if matches:
                return matches[0].strip()
            else:
                print("âš ï¸ æœªæ‰¾åˆ°mermaidå›¾ï¼Œè¿”å›å®Œæ•´å“åº”")
                return response
                
        except Exception as e:
            print(f"âŒ ç”ŸæˆMermaidå›¾å¤±è´¥: {e}")
            return ""
    
    def split_to_subsystems(self, mermaid_content: str) -> Dict[str, Any]:
        """
        æ ¹æ®Mermaidå›¾æ‹†åˆ†ä¸ºå­ç³»ç»Ÿ
        """
        try:
            # åˆ›å»ºåŒ…å«mermaidå†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨
            messages = split_messages.copy()
            messages.append({"role": "user", "content": mermaid_content})
            
            # è°ƒç”¨LLMæ‹†åˆ†å­ç³»ç»Ÿ
            response = llm_client.call(messages, "gpt-5-mini")
            
            # æå–JSONæ ¼å¼çš„å­ç³»ç»Ÿä¿¡æ¯
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                subsystems_data = json.loads(json_match.group(0))
                return subsystems_data
            else:
                print("âš ï¸ æœªæ‰¾åˆ°JSONæ ¼å¼çš„å­ç³»ç»Ÿä¿¡æ¯")
                return {"subsystems": []}
                
        except Exception as e:
            print(f"âŒ å­ç³»ç»Ÿæ‹†åˆ†å¤±è´¥: {e}")
            return {"subsystems": []}
    
    def generate_subprompts(self, original_text: str, subsystems_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºæ¯ä¸ªå­ç³»ç»Ÿç”Ÿæˆå¯¹åº”çš„æç¤ºè¯
        """
        try:
            # å‡†å¤‡æ¶ˆæ¯å†…å®¹
            user_content = f"<<<åˆå§‹æç¤ºè¯ï¼š\n{original_text}\nç”¨æˆ·åˆå§‹æç¤ºè¯ç»“æŸ>>>\n\n{json.dumps(subsystems_data, ensure_ascii=False)}"
            
            # åˆ›å»ºåŒ…å«å­ç³»ç»Ÿä¿¡æ¯çš„æ¶ˆæ¯åˆ—è¡¨
            messages = sub_prompt_messages.copy()
            messages.append({"role": "user", "content": user_content})
            
            # è°ƒç”¨LLMç”Ÿæˆå­æç¤ºè¯
            response = llm_client.call(messages, "gpt-5-mini")
            
            # æå–JSONæ ¼å¼çš„å­æç¤ºè¯ä¿¡æ¯
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                subprompts_data = json.loads(json_match.group(0))
                return subprompts_data
            else:
                print("âš ï¸ æœªæ‰¾åˆ°JSONæ ¼å¼çš„å­æç¤ºè¯ä¿¡æ¯")
                return {"subprompts": []}
                
        except Exception as e:
            print(f"âŒ å­æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}")
            return {"subprompts": []}
    
    def step2_split_to_subprompts(self, text_with_vars: str) -> Dict[str, Any]:
        """
        ç¬¬äºŒæ­¥ï¼šæ‹†åˆ†ä¸ºå­æç¤ºè¯
        æŒ‰ç…§first_split.pyçš„å®Œæ•´é€»è¾‘ï¼šmermaidç”Ÿæˆ â†’ å­ç³»ç»Ÿæ‹†åˆ† â†’ å­æç¤ºè¯ç”Ÿæˆ
        """
        print("=" * 50)
        print("ğŸ”€ ç¬¬äºŒæ­¥ï¼šæ‹†åˆ†ä¸ºå­æç¤ºè¯...")
        print("ğŸ“Š æŒ‰ç…§å®Œæ•´æµç¨‹ï¼šMermaidç”Ÿæˆ â†’ å­ç³»ç»Ÿæ‹†åˆ† â†’ å­æç¤ºè¯ç”Ÿæˆ")
        
        try:
            # 2.1 ç”ŸæˆMermaidæµç¨‹å›¾
            print("\nğŸ¨ æ­¥éª¤2.1ï¼šç”ŸæˆMermaidæµç¨‹å›¾...")
            mermaid_content = self.generate_mermaid_content(text_with_vars)
            if not mermaid_content:
                return {"error": "Mermaidå›¾ç”Ÿæˆå¤±è´¥"}
            print("âœ… Mermaidå›¾ç”Ÿæˆå®Œæˆ")
            
            # 2.2 æ‹†åˆ†ä¸ºå­ç³»ç»Ÿ
            print("\nğŸ”§ æ­¥éª¤2.2ï¼šæ‹†åˆ†ä¸ºå­ç³»ç»Ÿ...")
            subsystems_data = self.split_to_subsystems(mermaid_content)
            subsystems_count = len(subsystems_data.get("subsystems", []))
            if subsystems_count == 0:
                return {"error": "å­ç³»ç»Ÿæ‹†åˆ†å¤±è´¥"}
            print(f"âœ… æ‹†åˆ†å‡º {subsystems_count} ä¸ªå­ç³»ç»Ÿ")
            
            # 2.3 ç”Ÿæˆå­æç¤ºè¯
            print("\nğŸ“ æ­¥éª¤2.3ï¼šç”Ÿæˆå­ç³»ç»Ÿå¯¹åº”çš„æç¤ºè¯...")
            subprompts_data = self.generate_subprompts(text_with_vars, subsystems_data)
            subprompts_count = len(subprompts_data.get("subprompts", []))
            if subprompts_count == 0:
                return {"error": "å­æç¤ºè¯ç”Ÿæˆå¤±è´¥"}
            print(f"âœ… ç”Ÿæˆäº† {subprompts_count} ä¸ªå­æç¤ºè¯")
            
            # æ•´åˆç»“æœ
            result = {
                "method": "functional_split",
                "mermaid_content": mermaid_content,
                "subsystems": subsystems_data,
                "subprompts": subprompts_data,
                "processed_text": text_with_vars,
                "statistics": {
                    "subsystems_count": subsystems_count,
                    "subprompts_count": subprompts_count
                }
            }
            
            print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ")
            print(f"   - Mermaidå›¾: å·²ç”Ÿæˆ")
            print(f"   - å­ç³»ç»Ÿæ•°é‡: {subsystems_count}")
            print(f"   - å­æç¤ºè¯æ•°é‡: {subprompts_count}")
            
            return result
            
        except Exception as e:
            error_msg = f"æ‹†åˆ†æµç¨‹å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg}
    
    def step3_convert_to_cnlp(self, subprompts_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢ä¸ºCNLPæ ¼å¼
        è°ƒç”¨ nl2cnlp.py ä¸­çš„ç°æœ‰å‡½æ•°
        """
        print("=" * 50)
        print("ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢ä¸ºCNLPæ ¼å¼...")
        
        try:
            subprompts = subprompts_data.get("subprompts", {}).get("subprompts", [])
            if not subprompts:
                return {"error": "æ²¡æœ‰å­æç¤ºè¯å¯è½¬æ¢"}
            
            print(f"ğŸ¯ å¼€å§‹è½¬æ¢ {len(subprompts)} ä¸ªå­æç¤ºè¯...")
            
            # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡è½¬æ¢å‡½æ•°
            cnlp_results = batch_transform_cnlp(subprompts)
            
            # è¿‡æ»¤å‡ºæˆåŠŸè½¬æ¢çš„ç»“æœ
            successful_results = []
            failed_count = 0
            
            for i, result in enumerate(cnlp_results):
                if result and result.strip():
                    successful_results.append({
                        "index": i,
                        "name": subprompts[i].get("name", f"å­ç³»ç»Ÿ_{i+1}"),
                        "cnlp": result
                    })
                else:
                    failed_count += 1
                    print(f"âš ï¸ å­æç¤ºè¯ {i+1} è½¬æ¢å¤±è´¥")
            
            result = {
                "cnlp_results": successful_results,
                "total_count": len(subprompts),
                "success_count": len(successful_results),
                "failed_count": failed_count,
                "original_subprompts": subprompts_data
            }
            
            print(f"âœ… ç¬¬ä¸‰æ­¥å®Œæˆï¼ŒæˆåŠŸè½¬æ¢ {len(successful_results)}/{len(subprompts)} ä¸ªå­æç¤ºè¯")
            return result
            
        except Exception as e:
            error_msg = f"CNLPè½¬æ¢å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg}
    
    def run_complete_pipeline(self, 
                             input_file: str = 'nl_prompt.txt',
                             save_intermediate: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ‹†åˆ†æµç¨‹
        """
        print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„æç¤ºè¯æ‹†åˆ†æµç¨‹...")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        print("=" * 60)
        
        # è¯»å–åŸå§‹æç¤ºè¯
        original_text = self.read_file(input_file)
        if not original_text:
            return {"error": f"æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶ {input_file}"}
        
        print(f"ğŸ“Š åŸå§‹æ–‡æœ¬é•¿åº¦: {len(original_text)} å­—ç¬¦")
        
        # ç¬¬ä¸€æ­¥ï¼šæå–å˜é‡
        step1_result = self.step1_extract_variables(original_text)
        if "error" in step1_result:
            return step1_result
        
        if save_intermediate:
            self.save_json('output_step1_variables.json', step1_result)
            self.save_file('output_step1_text_with_vars.txt', step1_result['text_with_vars'])
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç…§first_split.pyçš„é€»è¾‘æ‹†åˆ†
        step2_result = self.step2_split_to_subprompts(step1_result['text_with_vars'])
        if "error" in step2_result:
            return step2_result
        
        if save_intermediate:
            self.save_json('output_step2_split.json', step2_result)
            # å•ç‹¬ä¿å­˜mermaidå›¾
            if step2_result.get('mermaid_content'):
                self.save_file('output_step2_mermaid.txt', step2_result['mermaid_content'])
        
        # ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢ä¸ºCNLP
        step3_result = self.step3_convert_to_cnlp(step2_result)
        if "error" in step3_result:
            return step3_result
        
        if save_intermediate:
            self.save_json('output_step3_cnlp.json', step3_result)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_result = {
            "step1_variables": step1_result,
            "step2_split": step2_result,
            "step3_cnlp": step3_result,
            "summary": {
                "input_file": input_file,
                "variables_count": len(step1_result.get('variables', [])),
                "subsystems_count": step2_result.get('statistics', {}).get('subsystems_count', 0),
                "subprompts_count": step2_result.get('statistics', {}).get('subprompts_count', 0),
                "cnlp_success_count": step3_result.get('success_count', 0),
                "cnlp_failed_count": step3_result.get('failed_count', 0)
            }
        }
        
        if save_intermediate:
            self.save_json('output_final_result.json', final_result)
        
        print("=" * 60)
        print("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"   - æå–å˜é‡æ•°é‡: {final_result['summary']['variables_count']}")
        print(f"   - å­ç³»ç»Ÿæ•°é‡: {final_result['summary']['subsystems_count']}")
        print(f"   - å­æç¤ºè¯æ•°é‡: {final_result['summary']['subprompts_count']}")
        print(f"   - CNLPè½¬æ¢æˆåŠŸ: {final_result['summary']['cnlp_success_count']}")
        print(f"   - CNLPè½¬æ¢å¤±è´¥: {final_result['summary']['cnlp_failed_count']}")
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("   - output_step1_variables.json: å˜é‡æå–ç»“æœ")
        print("   - output_step1_text_with_vars.txt: æ ‡è®°å˜é‡çš„æ–‡æœ¬")
        print("   - output_step2_split.json: å®Œæ•´æ‹†åˆ†ç»“æœ")
        print("   - output_step2_mermaid.txt: Mermaidæµç¨‹å›¾")
        print("   - output_step3_cnlp.json: CNLPè½¬æ¢ç»“æœ")
        print("   - output_final_result.json: å®Œæ•´ç»“æœ")
        
        return final_result


def main():
    """ä¸»å‡½æ•° - ç®€æ´çš„æµç¨‹æ§åˆ¶"""
    print("ğŸ¯ æç¤ºè¯æ‹†åˆ†ç³»ç»Ÿ - æµç¨‹ç¼–æ’å™¨")
    print("=" * 60)
    
    # åˆ›å»ºæµç¨‹ç¼–æ’å™¨å®ä¾‹
    pipeline = PromptSplitPipeline()
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    result = pipeline.run_complete_pipeline(
        input_file='nl_prompt.txt',
        save_intermediate=True
    )
    
    if "error" in result:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {result['error']}")
        return False
    else:
        print("âœ… æ‰€æœ‰æ­¥éª¤æ‰§è¡ŒæˆåŠŸï¼")
        return True


if __name__ == '__main__':
    success = main()
    exit(0 if success else 1) 