# PromptSplit - æ™ºèƒ½æç¤ºè¯æ‹†åˆ†ç³»ç»Ÿ

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

ä¸€ä¸ªæ™ºèƒ½çš„æç¤ºè¯æ‹†åˆ†å’Œç»“æ„åŒ–ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†å¤æ‚çš„è‡ªç„¶è¯­è¨€æç¤ºè¯è½¬æ¢ä¸ºå¯å¤ç”¨ã€æ¨¡å—åŒ–çš„æ¨¡æ¿ç³»ç»Ÿã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

PromptSplit æ˜¯ä¸€ä¸ªå®Œæ•´çš„æç¤ºè¯å·¥ç¨‹è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ—¨åœ¨è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
- å¤æ‚æç¤ºè¯éš¾ä»¥ç»´æŠ¤å’Œå¤ç”¨
- ç¼ºä¹æ ‡å‡†åŒ–çš„æç¤ºè¯ç»“æ„
- æ— æ³•æœ‰æ•ˆæ‹†åˆ†å¤§å‹æç¤ºè¯ç³»ç»Ÿ
- éœ€è¦å°†è‡ªç„¶è¯­è¨€æç¤ºè½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ” æ™ºèƒ½å˜é‡æå–**ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°æç¤ºè¯ä¸­çš„åŠ¨æ€å†…å®¹
- **ğŸ”€ å¤šç»´åº¦æ‹†åˆ†**ï¼šæ”¯æŒåŠŸèƒ½æ¨¡å—æ‹†åˆ†ï¼Œç”Ÿæˆæ¸…æ™°çš„ç³»ç»Ÿæ¶æ„
- **ğŸ“Š å¯è§†åŒ–åˆ†æ**ï¼šè‡ªåŠ¨ç”Ÿæˆ Mermaid æµç¨‹å›¾ï¼Œç›´è§‚å±•ç¤ºç³»ç»Ÿç»“æ„
- **ğŸ—ï¸ ç»“æ„åŒ–è¾“å‡º**ï¼šè½¬æ¢ä¸º CNLP (Controlled Natural Language Programming) æ ¼å¼
- **âš¡ å¹¶å‘å¤„ç†**ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ï¼Œæé«˜å¤„ç†æ•ˆç‡
- **ğŸ’¾ ä¸­é—´ç»“æœä¿å­˜**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½ä¼šä¿å­˜ä¸­é—´ç»“æœï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

## ğŸ”„ å·¥ä½œæµç¨‹

```mermaid
graph LR
    A[åŸå§‹æç¤ºè¯] --> B[æå–å˜é‡]
    B --> C[ç”ŸæˆMermaidå›¾]
    C --> D[æ‹†åˆ†å­ç³»ç»Ÿ]
    D --> E[ç”Ÿæˆå­æç¤ºè¯]
    E --> F[è½¬æ¢CNLPæ ¼å¼]
    F --> G[ç»“æ„åŒ–è¾“å‡º]
```

### è¯¦ç»†æ­¥éª¤

1. **ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½å˜é‡æå–**
   - æ–‡æœ¬åˆ†å—å¤„ç†ï¼Œæ”¯æŒå¤§æ–‡æœ¬
   - å¹¶å‘æå–æ˜¾å¼å’Œéšå¼å˜é‡
   - è‡ªåŠ¨å»é‡å’Œåå¤„ç†ä¼˜åŒ–

2. **ç¬¬äºŒæ­¥ï¼šç³»ç»Ÿæ‹†åˆ†**
   - ç”Ÿæˆ Mermaid æµç¨‹å›¾åˆ†æ
   - æ‹†åˆ†ä¸ºç‹¬ç«‹çš„å­ç³»ç»Ÿæ¨¡å—
   - ä¸ºæ¯ä¸ªå­ç³»ç»Ÿç”Ÿæˆä¸“é—¨çš„æç¤ºè¯

3. **ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼è½¬æ¢**
   - è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„ CNLP æ ¼å¼
   - ç”Ÿæˆå¯æ‰§è¡Œçš„ Agent å®šä¹‰
   - åŒ…å«è§’è‰²ã€çº¦æŸã€å·¥ä½œæµç¨‹ç­‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šWindows, macOS, Linux

### å®‰è£…ä¾èµ–

```bash
pip install json re concurrent.futures typing
```

### åŸºæœ¬ä½¿ç”¨

1. **å‡†å¤‡è¾“å…¥æ–‡ä»¶**
   ```bash
   # å°†æ‚¨çš„åŸå§‹æç¤ºè¯ä¿å­˜ä¸º nl_prompt.txt
   echo "æ‚¨çš„æç¤ºè¯å†…å®¹" > nl_prompt.txt
   ```

2. **è¿è¡Œæ‹†åˆ†ç³»ç»Ÿ**
   ```bash
   python run_split.py
   ```

3. **æŸ¥çœ‹ç»“æœ**
   ```bash
   # æ£€æŸ¥ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶
   ls output_*.json output_*.txt
   ```

## ğŸ“ é¡¹ç›®ç»“æ„

```
PromptSplit/
â”œâ”€â”€ run_split.py                    # ğŸ¯ ä¸»è¦çš„æµç¨‹ç¼–æ’å™¨
â”œâ”€â”€ extract_variable.py             # ğŸ” å˜é‡æå–æ¨¡å—
â”œâ”€â”€ first_spilit.py                 # ğŸ¨ Mermaidç”Ÿæˆå’Œæ‹†åˆ†æ¨¡å—
â”œâ”€â”€ nl2cnlp.py                      # ğŸ”„ CNLPè½¬æ¢æ¨¡å—
â”œâ”€â”€ LLMTool.py                      # ğŸ¤– LLM APIå®¢æˆ·ç«¯
â”œâ”€â”€ extract_var_v6.txt              # ğŸ“ å˜é‡æå–æç¤ºæ¨¡æ¿
â”œâ”€â”€ post_process_variable_v2.txt    # âœ¨ å˜é‡åå¤„ç†æç¤º
â”œâ”€â”€ nl_prompt.txt                   # ğŸ“¥ è¾“å…¥ï¼šåŸå§‹æç¤ºè¯
â”œâ”€â”€ sub_prompts.json                # ğŸ“‹ å­æç¤ºè¯ç¤ºä¾‹æ ¼å¼
â””â”€â”€ README.md                       # ğŸ“– ä½¿ç”¨è¯´æ˜
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

è¿è¡Œå®Œæˆåï¼Œç³»ç»Ÿä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

| æ–‡ä»¶å | æè¿° | å†…å®¹ç¤ºä¾‹ |
|--------|------|----------|
| `output_step1_variables.json` | å˜é‡æå–ç»“æœ | å˜é‡åˆ—è¡¨ã€ç»Ÿè®¡ä¿¡æ¯ |
| `output_step1_text_with_vars.txt` | æ ‡è®°å˜é‡çš„æ–‡æœ¬ | åŸæ–‡æœ¬ + `{å˜é‡}` æ ‡è®° |
| `output_step2_split.json` | å®Œæ•´æ‹†åˆ†ç»“æœ | å­ç³»ç»Ÿã€å­æç¤ºè¯ã€ç»Ÿè®¡ |
| `output_step2_mermaid.txt` | Mermaidæµç¨‹å›¾ | å¯æ¸²æŸ“çš„æµç¨‹å›¾ä»£ç  |
| `output_step3_cnlp.json` | CNLPè½¬æ¢ç»“æœ | ç»“æ„åŒ–Agentå®šä¹‰ |
| `output_final_result.json` | å®Œæ•´æµç¨‹ç»“æœ | æ‰€æœ‰æ­¥éª¤çš„æ±‡æ€»ç»“æœ |

### è¾“å‡ºç¤ºä¾‹

**å˜é‡æå–ç»“æœ (`output_step1_variables.json`)**
```json
{
  "variables": [
    "å®¢æˆ·æƒ…ç»ª", 
    "äº§å“ä¿¡æ¯", 
    "éœ€æ±‚æ¡ä»¶",
    "è¯é¢˜"
  ],
  "text_with_vars": "å¤„ç†{å®¢æˆ·æƒ…ç»ª}ç›¸å…³çš„{äº§å“ä¿¡æ¯}éœ€æ±‚...",
  "chunks_count": 3
}
```

**æ‹†åˆ†ç»“æœ (`output_step2_split.json`)**
```json
{
  "method": "functional_split",
  "mermaid_content": "flowchart TD\n  A[è¾“å…¥å¤„ç†] --> B[éœ€æ±‚åˆ†æ]\n  B --> C[å“åº”ç”Ÿæˆ]",
  "subsystems": {
    "subsystems": [
      {
        "name": "è¾“å…¥å¤„ç†æ¨¡å—",
        "responsibility": "å¤„ç†ç”¨æˆ·è¾“å…¥å’Œæ•°æ®é¢„å¤„ç†",
        "independence": "ç‹¬ç«‹çš„æ•°æ®å¤„ç†å±‚"
      }
    ]
  },
  "subprompts": {
    "subprompts": [
      {
        "name": "è¾“å…¥å¤„ç†å™¨",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¾“å…¥å¤„ç†å™¨...",
        "inputs": ["{ç”¨æˆ·è¾“å…¥}", "{å†å²è®°å½•}"],
        "outputs": ["{å¤„ç†ç»“æœ}", "{çŠ¶æ€ä¿¡æ¯}"]
      }
    ]
  },
  "statistics": {
    "subsystems_count": 3,
    "subprompts_count": 3
  }
}
```

**CNLPæ ¼å¼ (`output_step3_cnlp.json`)**
```json
{
  "cnlp_results": [
    {
      "index": 0,
      "name": "è¾“å…¥å¤„ç†å™¨",
      "cnlp": "[DEFINE_AGENT: InputProcessor \"è¾“å…¥å¤„ç†ä¸“å®¶\"]\n    [DEFINE_PERSONA:]\n        ROLE: ä¸“ä¸šçš„è¾“å…¥æ•°æ®å¤„ç†å’ŒéªŒè¯ä¸“å®¶\n    [END_PERSONA]\n    [DEFINE_WORKER: \"å¤„ç†ç”¨æˆ·è¾“å…¥\" ProcessInput]\n        [INPUTS]\n            REQUIRED <REF> ç”¨æˆ·è¾“å…¥ </REF>\n        [END_INPUTS]\n        [OUTPUTS]\n            REQUIRED <REF> å¤„ç†ç»“æœ </REF>\n        [END_OUTPUTS]\n        [MAIN_FLOW]\n            [SEQUENTIAL_BLOCK]\n                COMMAND-1 [COMMAND éªŒè¯è¾“å…¥æ ¼å¼...]\n            [END_SEQUENTIAL_BLOCK]\n        [END_MAIN_FLOW]\n    [END_WORKER]\n[END_AGENT]"
    }
  ],
  "success_count": 3,
  "failed_count": 0
}
```

## âš™ï¸ é…ç½®å’Œè‡ªå®šä¹‰

### LLMé…ç½®

åœ¨ `LLMTool.py` ä¸­é…ç½®æ‚¨çš„LLM APIï¼š

```python
# APIé…ç½®ç¤ºä¾‹
DEFAULT_API_URL = "https://api.your-llm-provider.com/v1/chat/completions"
DEFAULT_MODEL = "your-model-name"
API_KEY = "your-api-key"
```

### è‡ªå®šä¹‰å‚æ•°

æ‚¨å¯ä»¥é€šè¿‡ç¼–ç¨‹æ–¹å¼è‡ªå®šä¹‰è¿è¡Œå‚æ•°ï¼š

```python
from run_split import PromptSplitPipeline

# åˆ›å»ºæµç¨‹ç¼–æ’å™¨
pipeline = PromptSplitPipeline()

# è‡ªå®šä¹‰è¿è¡Œ
result = pipeline.run_complete_pipeline(
    input_file='custom_prompt.txt',    # è‡ªå®šä¹‰è¾“å…¥æ–‡ä»¶
    save_intermediate=True             # æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
)

# å•ç‹¬è¿è¡ŒæŸä¸ªæ­¥éª¤
step1_result = pipeline.step1_extract_variables(text)
step2_result = pipeline.step2_split_to_subprompts(text_with_vars)
step3_result = pipeline.step3_convert_to_cnlp(split_result)
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ‰¹é‡å¤„ç†

```python
import os
from run_split import PromptSplitPipeline

pipeline = PromptSplitPipeline()

# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
prompt_files = ['prompt1.txt', 'prompt2.txt', 'prompt3.txt']

for i, file in enumerate(prompt_files):
    print(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(prompt_files)}: {file}")
    result = pipeline.run_complete_pipeline(
        input_file=file,
        save_intermediate=True
    )
    # é‡å‘½åè¾“å‡ºæ–‡ä»¶é¿å…è¦†ç›–
    if "error" not in result:
        os.rename('output_final_result.json', f'output_final_result_{i+1}.json')
```

### é›†æˆåˆ°æ‚¨çš„é¡¹ç›®

```python
from run_split import PromptSplitPipeline

class YourPromptManager:
    def __init__(self):
        self.splitter = PromptSplitPipeline()
    
    def process_prompt(self, prompt_text: str) -> dict:
        # ä¸´æ—¶ä¿å­˜æç¤ºè¯
        with open('temp_prompt.txt', 'w', encoding='utf-8') as f:
            f.write(prompt_text)
        
        # å¤„ç†
        result = self.splitter.run_complete_pipeline(
            input_file='temp_prompt.txt',
            save_intermediate=False
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove('temp_prompt.txt')
        
        return result
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### è°ƒæ•´å¹¶å‘å‚æ•°

åœ¨ `extract_variable.py` ä¸­çš„ `process_chunks_concurrently` å‡½æ•°ï¼š

```python
# å¢åŠ å¹¶å‘çº¿ç¨‹æ•°ï¼ˆæ ¹æ®æ‚¨çš„ç¡¬ä»¶é…ç½®ï¼‰
results = process_chunks_concurrently(chunks, max_workers=10)
```

### è°ƒæ•´æ–‡æœ¬åˆ†å—å¤§å°

åœ¨å˜é‡æå–æ—¶ï¼š

```python
# æ ¹æ®æ‚¨çš„æ–‡æœ¬ç‰¹ç‚¹è°ƒæ•´åˆ†å—å¤§å°
chunks = split_text_by_length(text, chunk_size=300)  # é»˜è®¤500
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æç¤º"æ–‡ä»¶ä¸å­˜åœ¨"**
```bash
âŒ æ–‡ä»¶ nl_prompt.txt ä¸å­˜åœ¨
```
**A**: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºäº† `nl_prompt.txt` æ–‡ä»¶ï¼Œå¹¶åŒ…å«æ‚¨è¦å¤„ç†çš„æç¤ºè¯å†…å®¹ã€‚

**Q: APIè°ƒç”¨å¤±è´¥**
```bash
âŒ è°ƒç”¨APIæ—¶å‘ç”Ÿé”™è¯¯: Connection timeout
```
**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
- ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- `LLMTool.py` ä¸­çš„APIé…ç½®æ˜¯å¦æ­£ç¡®
- APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
- APIæœåŠ¡æ˜¯å¦å¯ç”¨

**Q: å˜é‡æå–ç»“æœä¸ºç©º**
```bash
ğŸ¯ æå–åˆ° 0 ä¸ªå˜é‡: []
```
**A**: å¯èƒ½çš„åŸå› ï¼š
- è¾“å…¥æ–‡æœ¬è¿‡çŸ­æˆ–ä¸åŒ…å«å¯æå–çš„å˜é‡
- `extract_var_v6.txt` æç¤ºæ¨¡æ¿éœ€è¦ä¼˜åŒ–
- LLMæ¨¡å‹ç†è§£èƒ½åŠ›é™åˆ¶

**Q: Mermaidå›¾ç”Ÿæˆå¤±è´¥**
```bash
âš ï¸ æœªæ‰¾åˆ°mermaidå›¾ï¼Œè¿”å›å®Œæ•´å“åº”
```
**A**: è¿™é€šå¸¸æ˜¯LLMè¾“å‡ºæ ¼å¼é—®é¢˜ï¼š
- æ£€æŸ¥ `first_spilit.py` ä¸­çš„æç¤ºæ¨¡æ¿
- å°è¯•ä¸åŒçš„LLMæ¨¡å‹
- æ£€æŸ¥è¾“å…¥æ–‡æœ¬çš„å¤æ‚åº¦

**Q: å­ç³»ç»Ÿæ‹†åˆ†å¤±è´¥**
```bash
âŒ å­ç³»ç»Ÿæ‹†åˆ†å¤±è´¥: No valid JSON found
```
**A**: JSONè§£æé”™è¯¯ï¼š
- LLMè¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ
- å¯ä»¥æ£€æŸ¥ä¸­é—´è¾“å‡ºæ–‡ä»¶æ’æŸ¥é—®é¢˜
- å°è¯•ç®€åŒ–è¾“å…¥æ–‡æœ¬

### è°ƒè¯•æ¨¡å¼

æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼š

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# åœ¨ä¸»å‡½æ•°ä¸­æ·»åŠ 
if __name__ == '__main__':
    try:
        success = main()
        exit(0 if success else 1)
    except Exception as e:
        logging.error(f"ç³»ç»Ÿå¼‚å¸¸: {e}", exc_info=True)
        exit(1)
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. è¾“å…¥æ–‡æœ¬å‡†å¤‡
- **æ¸…æ™°ç»“æ„**: ç¡®ä¿åŸå§‹æç¤ºè¯æœ‰æ¸…æ™°çš„æ®µè½å’Œé€»è¾‘ç»“æ„
- **é€‚å½“é•¿åº¦**: å»ºè®®å•ä¸ªæç¤ºè¯é•¿åº¦åœ¨1000-5000å­—ç¬¦ä¹‹é—´
- **é¿å…æ ¼å¼é—®é¢˜**: ä½¿ç”¨UTF-8ç¼–ç ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦

### 2. å˜é‡å‘½å
- ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å
- é¿å…è¿‡äºé€šç”¨çš„è¯æ±‡ï¼ˆå¦‚"å†…å®¹"ã€"ä¿¡æ¯"ï¼‰
- ä¿æŒä¸€è‡´çš„å‘½åé£æ ¼

### 3. ç»“æœéªŒè¯
- æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºè´¨é‡
- éªŒè¯ç”Ÿæˆçš„å­æç¤ºè¯æ˜¯å¦ä¿æŒåŸæ–‡è¯­ä¹‰
- ç¡®è®¤CNLPæ ¼å¼çš„æ­£ç¡®æ€§

### 4. æ€§èƒ½ä¼˜åŒ–
- å¯¹äºå¤§æ–‡æœ¬ï¼Œé€‚å½“è°ƒæ•´åˆ†å—å¤§å°
- æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´å¹¶å‘çº¿ç¨‹æ•°
- å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/PromptSplit.git
cd PromptSplit

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è¿è¡Œæµ‹è¯•
python -m pytest tests/
```

### æäº¤è§„èŒƒ

- éµå¾ª [Conventional Commits](https://conventionalcommits.org/) è§„èŒƒ
- ç¡®ä¿ä»£ç é€šè¿‡æ‰€æœ‰æµ‹è¯•
- æ·»åŠ å¿…è¦çš„æ–‡æ¡£è¯´æ˜

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æº - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…çš„æ”¯æŒ
- åŸºäºå…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯
- å‚è€ƒäº†å¤šä¸ªå¼€æºé¡¹ç›®çš„è®¾è®¡ç†å¿µ

## ğŸ“ è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: [GitHub Repository](https://github.com/your-username/PromptSplit)
- é—®é¢˜åé¦ˆ: [Issues](https://github.com/your-username/PromptSplit/issues)
- è®¨è®ºäº¤æµ: [Discussions](https://github.com/your-username/PromptSplit/discussions)

---

**å¿«é€Ÿå¼€å§‹ç¤ºä¾‹**

```bash
# 1. å‡†å¤‡æ‚¨çš„æç¤ºè¯
echo "æ‚¨çš„å¤æ‚æç¤ºè¯å†…å®¹..." > nl_prompt.txt

# 2. è¿è¡Œæ‹†åˆ†ç³»ç»Ÿ
python run_split.py

# 3. æŸ¥çœ‹ç»“æœ
cat output_final_result.json | python -m json.tool
```

ğŸ‰ **ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰é—®é¢˜è¯·éšæ—¶åé¦ˆã€‚** 